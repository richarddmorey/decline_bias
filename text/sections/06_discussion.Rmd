
The bias due to coining in meta-analyses was previously pointed out by @franklinBiasEvaluationBias2017 [see also commentary by @sterneCommentaryDoesSelective2018], but does not appear to be widely known. Currently, @franklinBiasEvaluationBias2017's excellent commentary has fewer than twenty citations according to Scopus, and meta-analysts continue to use the technique. @morrisseyMetaanalysisMagnitudesDifferences2016 discusses na√Øve meta-analyses of absolute values and other nonlinear transformations of data and points out that meta-analytic "findings" may be mere artifacts of poor statistical models. Here I show that coining introduces these kinds of artifacts, and DRPs may introduce them as well.

Although the problem with coining (and related data transforms) have been noted before in other literatures, the point that the innocuous DRPs may cause equivalent effects appears to be novel. This raises the general possibility that the simple assumptions underlying most meta-analyses---that effect size reports can be treated as independent observations with symmetric error around a fixed true value---may often be false in important ways even when there is no problematic behaviour among scientists.

How much does this effect any particular meta-analysis? This is unknown, and I am not arguing here that this artifact accounts for any particular previous finding. Other issues (e.g. publication bias) can also cause the effects, and not all research contexts will be as susceptible to sign alignment. I do, however, take it for granted that meta-analysts should not use methods *known* to be biased. If a meta-analyst wants to draw an inference in any research context, the burden of evidence is squarely on them to argue that their inference is not plagued by artifacts such as those from sign-alignment or other DRPs. The use and interpretation of methods that may be sensitive to the issues I describe either implicitly or explicitly assumes that they are *not* a problem.

It has been argued that many experiments in psychological science have effect sizes that are small enough to lead to a high probability that a replication may invert the sign merely by chance (see e.g. @OpenScienceCollaboration:2015, which reported that 17 of 97 replications showed an opposite sign to the initial study). Small effects with reports depending on the data will lead to artifacts when studying decline effects. Interestingly, the reformers using the biased methods in meta-analyses are the ones that argue that the conditions exist that would lead to maximum bias.

## Recommendations

**Meta-analysis should never coin effect sizes.** I believe it should go without saying that meta-analysts should not introduce a source of bias into their meta-analyses, even if it appears to improve interpretability. Meta-analysts must find a way outside the data itself to align effect sizes. In some cases this will be straightforward (e.g., treatment/control designs or based on theory); in others, it will be less straightforward. In any case, however, it seems that meta-analysts cannot avoid engaging with the research contexts of the results they study.

**Meta-analysts should not include the initial study in most meta-analyses.** If it is uncertain whether an initial report was affected by DRPs, it is simply safer to exclude it. The simple act of excluding the initial study will entirely mitigate the effect I have pointed out, because it is caused by the error in the initial study. Exclusion of the initial study would also mitigate regression to the mean [sometimes called the "winner's curse", @youngWhyCurrentPublication2008], which seems unavoidable otherwise. 

**Meta-analysts should seek to understand the role of DRPs in meta-analysis.** As I have shown, reporting practices that have no effect on the inferences in individual studies may nevertheless have an effect on meta-analyses. If the the simple practice of reporting a novel effect in the direction in which it is observed can bias meta-analyses---and this has remained unexplored for decades---this raises the possibility that other DRPs may effect meta-analyses.

As I have previously argued [@moreyPoorStatisticalProperties2025], it is crucial for the health of science that meta-scientific tools should have good properties (e.g. not find evidence for poor behaviour) when scientific behaviour is not problematic. We do not have to agree on every aspect of good or poor scientific behavior to agree that methods yielding artifactual decline effects should not be used. If meta-analysts continue to use biased methods, the rhetoric used to attack poor-quality science now can (and will) be used against good-quality science in the future. 




